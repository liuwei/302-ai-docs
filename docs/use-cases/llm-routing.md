# LLM Routing Strategies

LLM routing is the process of dynamically selecting an AI model or provider
based on predefined criteria such as cost, latency, reliability, or task suitability.

It is a core capability of unified AI infrastructure.

---

## Common Routing Strategies

### Cost-Based Routing
- Selects the lowest-cost model that meets quality requirements

### Latency-Based Routing
- Chooses the fastest responding provider in real time

### Capability-Based Routing
- Routes requests based on model strengths (e.g. reasoning vs creativity)

### Fallback Routing
- Automatically switches providers on failure or timeout

---

## Why Routing Matters
Routing improves system resilience, reduces operational cost,
and enables multi-model optimization in production environments.

---

## When to Use LLM Routing
- Multi-provider systems
- Cost-sensitive workflows
- Reliability-critical workloads

See also:
- [System Architecture](../system-architecture.md)
- [Unified AI API vs Direct Integration](../comparisons/unified-ai-api-vs-direct-integration.md)

